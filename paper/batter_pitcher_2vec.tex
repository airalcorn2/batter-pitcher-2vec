\documentclass{article}

\usepackage{adjustbox}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{batter_pitcher_2vec}
\usepackage[doi=false,
            eprint=false,
            isbn=false,
            maxbibnames=99,
            sorting=none]{biblatex}        % bibliography
\usepackage{booktabs}       % professional-quality tables
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{mathtools}
\usepackage{microtype}      % microtypography
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{url}            % simple URL typesetting

\addbibresource{batter_pitcher_2vec.bib}

\title{(batter|pitcher)2vec: better at-bat modeling with neural player embeddings}
\date{}

\author{
    Michael A. Alcorn\thanks
    {Personal website: \url{https://sites.google.com/view/michaelaalcorn}\newline
    \hspace*{1.8em}Code: \url{https://github.com/airalcorn2/batter-pitcher-2vec}} \\
    Red Hat, Inc.\\
    Chicago, IL 60615 \\
    \texttt{michaelaalcorn@gmail.com} \\
}

\begin{document}

\maketitle

\begin{abstract}

This paper introduces \texttt{(batter|pitcher)2vec}, a neural network algorithm inspired by \texttt{word2vec} that learns distributed representations of Major League Baseball players. The representations are discovered through a supervised learning task that attempts to predict the outcome of an at-bat (e.g., strike out, home run) given a specific batter and pitcher. The learned representations qualitatively appear to better reflect baseball intuition than traditional baseball statistics, for example, by grouping pitchers together who rely primarily on pitches with dramatic movement. Further, like \texttt{word2vec}, the representations possess intriguing algebraic properties, for example, capturing the fact that Bryce Harper might be considered Mike Trout's left-handed dopplegänger. Lastly, \texttt{(batter|pitcher)2vec} is significantly more accurate at modeling future at-bat outcomes for previously unseen matchups than simpler approaches.

\end{abstract}

\section{Introduction}

Baseball is notorious for its culture of statistical bookkeeping. The depth and breadth of baseball statistics allows fans and professionals alike to compare players and forecast outcomes with varying degrees of accuracy. Although many traditional baseball statistics can be quite informative and useful, they can also be somewhat abitrary and thus may not accurately reflect the true talent of any given player. The field of Sabermetrics developed in an effort to address some of the inherent limitations of standard baseball statistics. For example, Wins Above Replacement (WAR) "offers an estimate to answer the question, 'If this player got injured and their team had to replace them with a freely available minor leaguer or a AAAA player from their bench, how much value would the team be losing?'" \parencite{WAR}. However, the WAR formula (\ref{eqn:war}) is, itself, somewhat ad hoc, reflecting the intuition of the statistic's designer(s):

\begin{equation}
\label{eqn:war}
WAR = \frac{BR + BRR + FR + PA + LA + RR}{RPW}
\end{equation}

where $BR$ is batting runs, $BRR$ is base running runs, $FR$ is fielding runs, $PA$ is a positional adjustment, $LA$ is a league adjustment, $RR$ is replacement runs, and $RPW$ is runs per win.

Whereas the WAR statistic uses a combination of conventional baseball statistics to quantify an individual player's impact on his team's overall record, the Player Empirical Comparison and Optimization Test Algorithm (PECOTA) attempts to estimate a player's true talent by identifying a neighborhood of players with similar statistics and performing an age correction \parencite{PECOTA}; but, again, the method fundamentally depends on traditional aggregate statistics.

When scouts assess the talent of a player, they do not simply count up the number of times the player made it to first base or struck out. They consider the player's tendencies in a number of different contexts. For example, a scout might notice that a certain left-handed batter tends to ground out to third base when facing left-handed curveball pitchers. Or that a certain right-handed fastball pitcher tends to struggle against short right-handed batters. An algorithm capable of learning these subtleties would be a valuable tool for assessing player talent.

The task of extracting informative measures of talent in Major League Baseball (MLB) players has a surprising parallel in the field of natural language processing, word embedding modeling. Words, like MLB players, can be considered distinct elements in a set, and one common way to represent categorical data in machine learning algorithms is with one-hot encodings. A one-hot encoding is an $N$-dimensional vector (where $N$ is the number of elements in the set) consisting entirely of zeros except for a single one at the location corresponding to the element's index. For example, the one-hot encodings for a vocabulary consisting of the words \{"dog", "cat", "constitutional"\} might be $\begin{bsmallmatrix} 1 & 0 & 0 \end{bsmallmatrix}$, $\begin{bsmallmatrix} 0 & 1 & 0 \end{bsmallmatrix}$, and $\begin{bsmallmatrix} 0 & 0 & 1 \end{bsmallmatrix}$, respectively.

One drawback of one-hot encodings is that each vector in the set is orthogonal to and equidistant from every other vector in the set; i.e., every element in the set is equally similar (or dissimilar) to every other element in the set. Words, however, do exhibit varying degrees of semantic similarity. For example, the words "dog" and "cat" are clearly semantically more similar than the words "dog" and "constitutional". Word embeddings are a technique for mathematically encoding such similarities as geometric properties (e.g., cosine similarity or Euclidean distance), and the use of these representations generally leads to more computationally efficient learning algorithms \parencite{Bengio2003}.

Perhaps the most famous word embedding algorithm is \texttt{word2vec} \parencite{Mikolov2013}, a neural network that learns distributed word representations in a supervised setting. The \texttt{word2vec} algorithm can take two different forms: (1) the continuous bag-of-words model (CBOW) and (2) the skip-gram model (see Figure~\ref{fig:word2vec}). The CBOW model attempts to predict a word ($w_t$) given some surrounding words (e.g., $w_{t-1}$ and $w_{t+1}$, although the context can be larger). In contrast, the skip-gram model attempts to predict the surrounding words given some central word.

\begin{figure}[h]
\captionsetup[subfigure]{labelformat=empty}
\centering

    \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=5cm]{cbow_fig.png}
    \caption{}
    \end{subfigure}%
    \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=5cm]{skip_gram.png}
    \caption{}
    \end{subfigure}

\caption{The CBOW (left) and skip-gram (right) model architectures for a window of three words. A one-hot vector is depicted as a black square among an array of white square while an embedding is depicted as an array of gray squares.}
\label{fig:word2vec}
\end{figure}

In this paper, I introduce \texttt{(batter|pitcher)2vec}, an algorithm that adapts representation learning concepts (like those found in \texttt{word2vec}) to a baseball setting. Specifically, the model learns to predict the outcome of an at-bat given a specific batter and pitcher. Unlike many Sabermetrics statistics, \texttt{(batter|pitcher)2vec} learns from "raw" baseball events as opposed to aggregate statistics, which allows it to incorporate additional contextual information when modeling player talent.

\section{Data}
\label{data}

\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{ | l | l | l | l | }
    \hline
    Symbol & Description & Symbol & Description \\ 
    \hline\hline
    1-9 & Out first handled by fielder \# & E1-E9 & Error first handled by fielder \# \\
    \hline
    S1-S9 & Single first handled by fielder \# & K & Strike out \\
    \hline
    D1-D9 & Double first handled by fielder \# & W & Walk \\
    \hline
    DG & Ground rule double & IW & Intentional walk \\
    \hline
    T1-T9 & Triple first handled by fielder \# & BK & Balk \\
    \hline
    HR & Home run & HP & Hit by a pitch \\
    \hline
    \end{tabular}
\end{adjustbox}
\caption{The possible at-bat outcomes.}
\label{table:at_bats}
\end{table}

Play-by-play data for each game from the 2013, 2014, 2015, and 2016 seasons were obtained from the Retrosheet website \parencite{Retro}. Each play description was converted into a tuple consisting of the batter, pitcher, and at-bat outcome, for example, (Mike Trout, Anthony Bass, HR) (where HR is the symbol denoting a home run). There are 52 potential play outcomes (see Table~\ref{table:at_bats}), but the model only considers 49 different outcomes because there were no observed instances of doubles first fielded by the catcher, triples first fielded by the catcher, or triples first fielded by the third baseman. The raw training data set consisted of 557,436 at-bats from the 2013, 2014, and 2015 seasons representing 1,634 different batters and 1,226 different pitchers. To ensure an adequate amount of data was available for learning player representations, only the most frequent batters and pitchers involved in 90\% of at-bats were included in the analysis. The final data set consisted of 461,231 at-bats representing 524 batters and 565 pitchers.

\section{Model}
\label{model}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth,height=\textheight,keepaspectratio]{batter_pitcher_model.png}
\caption{The \texttt{(batter|pitcher)2vec} model architecture.}
\label{fig:batter_pitcher}
\end{figure}

The \texttt{(batter|pitcher)2vec} model architecture can be seen in Figure~\ref{fig:batter_pitcher}, and the similarities between \texttt{(batter|pitcher)2vec} and \texttt{word2vec} should be readily apparent. The model takes one-hot encodings of a batter and pitcher as input and selects the player embeddings from the the batter (\ref{eqn:batter_embedding}) and pitcher (\ref{eqn:pitcher_embedding}) embedding matrices, respectively.

\begin{equation}
\label{eqn:batter_embedding}
w_b = W^B \cdot h_b
\end{equation}

\begin{equation}
\label{eqn:pitcher_embedding}
w_p = W^P \cdot h_p
\end{equation}

Here, $h_b$ is the $N_B$-dimensional (where $N_B$ is the number of batters) one-hot vector for the batter indexed at $b$, $W^B$ is the batter embedding matrix, and $w_b$ is the batter's embedding (i.e., the column in $W^{B}$ indexed by $b$, $W^{B}_{:,b}$). Likewise, $h_p$ is the $N_P$-dimensional (where $N_P$ is the number of pitchers) one-hot vector for the pitcher indexed at $p$, $W^P$ is the pitcher embedding matrix, and $w_p$ is the pitcher's embedding.

The batter and pitcher embeddings are then concatenated together (\ref{eqn:concat}) and fed into a standard softmax layer (\ref{eqn:output} and \ref{eqn:prob}), which outputs a probability distribution over at-bat outcomes.

\begin{equation}
\label{eqn:concat}
w_{b \oplus p} = w_b \oplus w_p
\end{equation}

\begin{equation}
\label{eqn:output}
z = W_o \cdot w_{b \oplus p} + b_o
\end{equation}

\begin{equation}
\label{eqn:prob}
p(o_i | h_b, h_p) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\end{equation}

Here, $o_i$ is the at-bat outcome indexed by $i$ and $K$ is the number of at-bat outcomes.

Maximizing the likelihood of this model is equivalent to minimizing the cross entropy (\ref{eqn:entropy}), which is the standard loss function used in machine learning classification tasks:

\begin{equation}
\label{eqn:entropy}
H(p,q) = \sum_{x} p(x)log(q(x))
\end{equation}

where $p$ is the true distribution and $q$ is the predicted distribution.

The model was implemented in Keras 2.0 \parencite{Keras2015} and trained on a laptop with 16 gigabytes of RAM and an Intel i7 CPU. The model was optimized using Nesterov accelerated mini-batch gradient descent with the learning rate, momentum, and decay hyperparameters set at $0.01$, $0.9$, and $10^{-6}$, respectively. The model was trained for $100$ epochs and with mini-batches of $100$ samples. The embeddings for both batters and pitchers were nine-dimensional. The code to generate the results described in this paper can be found at \url{https://github.com/airalcorn2/batter-pitcher-2vec}.

\section{Results}
\label{results}

\subsection{Visual inspection of player embeddings}

A visual inspection of the embeddings can provide some intuition for what the model was learning. Two-dimensional embeddings of the batter and pitcher embeddings were obtained using the t-SNE algorithm \parencite{VanderMaaten2008} and can be seen in Figure~\ref{fig:tsne}. Intriguing player clusterings are readily apparent, for example, Mike Trout and Paul Goldschmidt, Dee Gordon and Ichiro Suzuki, and Aroldis Chapman and Dellin Betances are all close pairs.

\begin{figure}
\captionsetup[subfigure]{labelformat=empty}
\centering

    \begin{subfigure}[b]{0.75\textwidth}
    \includegraphics[width=1\linewidth]{batter_tsne.png}
    \caption{}
    \end{subfigure}

    \begin{subfigure}[b]{0.75\textwidth}
    \includegraphics[width=1\linewidth]{pitcher_tsne.png}
    \caption{}
    \end{subfigure}

\caption{Two-dimensional t-SNE embeddings of the batter embeddings (top) and pitcher embeddings (bottom).}
\label{fig:tsne}
\end{figure}

Coloring batters according to various properties and plotting them on the first two principal components of a principal component analysis of the embeddings reveals further insights (Figure~\ref{fig:batter_traits}). For example, left-handed hitters are clearly distinguishable from right-handed hitters, and batters with high single rates exist at the opposite end of batters with low single rates (a similar pattern is visible for home run rates). At the least, \texttt{(batter|pitcher)2vec} is capable of reproducing the information contained in standard baseball statistics.

\begin{figure}
\captionsetup[subfigure]{labelformat=empty}
\centering
    \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{batter_single.png}
    \caption{}
    \end{subfigure}%
    \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{batter_hr.png}
    \caption{}
    \end{subfigure}\\[1ex]

    \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{batter_hand.png}
    \caption{}
    \end{subfigure}
\caption{A plot of the first two principal components of the batter embeddings colored by (clockwise from the top left): single rate, home run rate, and handedness.}
\label{fig:batter_traits}
\end{figure}

\subsection{Nearest neighbors}

Further insight can be obtained by inspecting the neighborhoods of different players. As suggested by the t-SNE figure, Paul Goldschmidt is Mike Trout's nearest neighbor; an unsurprising result considering how each athlete is known for his rare blend of speed and power \parencite{Kory2015}. Similarly, Ichiro Suzuki is Dee Gordon's nearest neighbor, and both have a reputation for being able to get on base \parencite{Sullivan2015}. Notably, when clustering players using common MLB stats (e.g., HRs, RBIs), Paul Goldschmidt is not among Mike Trout's ten nearest neighbors, nor is Ichiro Suzuki among Dee Gordon's ten nearest neighbors.

On the pitching side, Craig Kimbrel is Aroldis Chapman's nearest neighbor, which is unsurprising considering both are known as elite closers with overpowering fastballs \parencite{Mirsky2016}. Other nearest neighbors are not as immediately intuitive, for example, Clayton Kershaw and Craig Stammen and Sonny Gray and Jake Arrieta, but a method like \texttt{(batter|pitcher)2vec} could potentially reveal surprising similarities between players who are not considered similar by human standards.

\subsection{Opposite-handed dopplegängers}

One of the most fascinating properties of effective word embeddings is their analogy capabilities \parencite{Mikolov2013a}. For example, when using \texttt{word2vec} word embeddings, subtracting the vector for "France" from the vector for "Paris" and then adding the vector for "Rome" produces a vector that is very close to the vector for Rome, which corresponds to the analogy Paris : France :: Rome : Italy \parencite{Mikolov2013a}.

A similar type of algebra may be possible with the \texttt{(batter|pitcher)2vec} embeddings. For example, one might expect that subtracting the average left-handed batter embedding from a right-handed batter's embedding would produce that player's left-handed dopplegänger. In fact, subtracting the average left-handed batter embedding from Mike Trout's embedding produces a vector with Chris Davis, David Ortiz, and Bryce Harper as the three nearest neighbors, suggesting the algebra hypothesis holds (see \parencite{Spector2016} for a discussion comparing Mike Trout and Bryce Harper). Similarly, Tyler Saladino appears to be an appropriate candidate for Dee Gordon's right-handed dopplegänger \parencite{Chamberlain2017}.

\subsection{Previously unseen matchup modeling}

The representations learned by neural networks are theoretically interesting because they suggest the models are learning "causal" factors from inputs when they are able to generalize (or transfer) well \parencite{RepresentationLearning}. In the case of \texttt{(batter|pitcher)2vec}, accurately modeling at-bat outcome distributions for unseen batter and pitcher pairs might indicate that the model is extracting truly meaningful baseball qualities during learning. To test the model's performance, at-bat outcomes were collected from the 2016 season for previously unseen matchups that included batters and pitchers from the training set. In all, there were 21,479 previously unseen matchups corresponding to 51,580 at-bats.

A naïve prediction strategy will be used as a baseline for comparison to \texttt{(batter|pitcher)2vec}. For any given batter, the expected outcome distribution will be defined as:

\begin{equation}
\label{eqn:batter_naïve}
p(o_i|b_j)=\frac{c_{o_{i,j}} + r_{o_i}}{\sum_{k=1}^{K} c_{o_{j,k}} + 1}
\end{equation}

where $o_i$ denotes the outcome indexed by $i$, $b_j$ represents the batter indexed by $j$, $c_{o_{i,j}}$ is the number of times the batter indexed by $j$ had an at-bat resulting in the outcome indexed by $i$ in the training data, $r_{o_i}$ is the proportion of all at-bats that resulted in the outcome indexed by $i$ in the training data, and $K$ is the number of outcomes. Essentially, the procedure adds one at-bat to each batter's data, but distributes the mass of that single at-bat across all possible outcomes based on data from all batters. $r_{o_i}$ can thus be considered a type of "prior" or smoothing factor. $p(o_i|t_j)$ will be similarly defined for pitchers. The naïve expected outcome distribution for a given batter ($b_j$) and pitcher ($t_k$) matchup is thus defined as:

\begin{equation}
\label{eqn:naïve}
p(o_i|b_j,t_k) = \frac{p(o_i|b_j) + p(o_i|t_k)}{2}
\end{equation}

The cross entropy can then be obtained for each at-bat in the test set using both the naïve approach and \texttt{(batter|pitcher)2vec}. The naïve approach produces an average cross entropy of $2.8113$ on the test set while \texttt{(batter|pitcher)2vec} produces a significantly ($p < 0.001$) lower average cross entropy of $2.7848$, a $0.94\%$ improvement. But is an improvement of only $0.94\%$ over the baseline particularly impressive? Comparing the performance of \texttt{(batter|pitcher)2vec} to that of a multinomial logistic regression model (Figure~\ref{fig:log_reg}) may provide some insight. In fact, the multinomial logistic regression model performs slightly worse than the naïve approach with an average cross entropy of $2.8118$. \texttt{(batter|pitcher)2vec)} thus appears to be exceptional in its ability to model at-bat outcome distributions for previously unseen matchups.

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth,height=\textheight,keepaspectratio]{logistic_regression.png}
\caption{A graphical model depiction of a multinomial logistic regression model.}
\label{fig:log_reg}
\end{figure}

\section{Future directions}
\label{future}

The results presented in this paper should encourage further exploration of similar neural network architectures in sports settings. Modifying \texttt{(batter|pitcher)vec} to predict PITCHf/x measurements as opposed to discrete outcomes could be fruitful as PITCHf/x data would likely convey more information about a player's physical characteristics. Another potentially interesting adaptation of \texttt{(batter|pitcher)vec} would be including the pitcher's team as one of the inputs to model as a way of modeling the pitcher's supporting defense.

Applying \texttt{(batter|pitcher)vec} to minor league baseball players would allow one to then train a second model that learns to map a player's minor league representation to his MLB representation. Such a model could then be used for scouting purposes by mapping minor league players into the MLB space and investigating the neighborhood of the prospect's mapped embedding.

Interesting applications of \texttt{(batter|pitcher)vec}-like architectures in sports other than baseball are also likely possible, for example, modeling National Fooball League offenses and defenses as proposed by \parencite{Alcorn2016}.

\subsubsection*{Acknowledgments}

I would like to thank my colleague Erik Erlandson for his suggestion to investigate opposite-handed dopplegängers.

\printbibliography

\end{document}