Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Keras2015,
author = {Chollet, Fran{\c{c}}ois},
publisher = {GitHub},
title = {{Keras}},
url = {https://github.com/fchollet/keras},
year = {2015}
}
@article{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector repre-sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ-ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor-mance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {cs.CL/1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1301.3781},
journal = {ArXiv e-prints},
keywords = {Computer Science - Computation and Language},
primaryClass = {cs.CL},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {https://arxiv.org/pdf/1301.3781.pdf},
year = {2013}
}
@inproceedings{Alex2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Alex, Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Neural Information Processing Systems (NIPS)},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@misc{Chamberlain2017,
author = {Chamberlain, Alex},
booktitle = {FanGraphs},
title = {{All Aboard the Tyler Saladino Hype Train}},
url = {http://www.fangraphs.com/fantasy/all-aboard-the-tyler-saladino-hype-train/},
urldate = {2017-04-19},
year = {2017}
}
@misc{Sullivan2015,
author = {Sullivan, Jeff},
booktitle = {FanGraphs},
title = {{Dee Gordon Has Been Going Full Ichiro}},
url = {http://www.fangraphs.com/blogs/dee-gordon-has-been-going-full-ichiro/},
urldate = {2017-04-19},
year = {2015}
}
@incollection{RepresentationLearning,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaaron},
booktitle = {Deep Learning},
chapter = {15},
pages = {526--557},
publisher = {MIT Press},
title = {{Representation Learning}},
url = {http://www.deeplearningbook.org/contents/representation.html},
year = {2016}
}
@misc{PECOTA,
author = {Silver, Nate},
booktitle = {Baseball Prospectus},
title = {{Baseball Prospectus Basics: The Science of Forecasting}},
url = {http://www.baseballprospectus.com/article.php?articleid=2659},
urldate = {2017-04-10},
year = {2004}
}
@article{Pascanu2013,
abstract = {This paper explores the complexity of deep feedforward networks with linear pre-synaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piecewise linear functions based on computational geometry. We look at a deep rectifier multi-layer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime, when the number of inputs stays constant, if the shallow model has {\$}kn{\$} hidden units and {\$}n{\_}0{\$} inputs, then the number of linear regions is {\$}O(k{\^{}}{\{}n{\_}0{\}}n{\^{}}{\{}n{\_}0{\}}){\$}. For a {\$}k{\$} layer model with {\$}n{\$} hidden units on each layer it is {\$}\backslashOmega(\backslashleft\backslashlfloor {\{}n{\}}/{\{}n{\_}0{\}}\backslashright\backslashrfloor{\^{}}{\{}k-1{\}}n{\^{}}{\{}n{\_}0{\}}){\$}. The number {\$}\backslashleft\backslashlfloor{\{}n{\}}/{\{}n{\_}0{\}}\backslashright\backslashrfloor{\^{}}{\{}k-1{\}}{\$} grows faster than {\$}k{\^{}}{\{}n{\_}0{\}}{\$} when {\$}n{\$} tends to infinity or when {\$}k{\$} tends to infinity and {\$}n \backslashgeq 2n{\_}0{\$}. Additionally, even when {\$}k{\$} is small, if we restrict {\$}n{\$} to be {\$}2n{\_}0{\$}, we can show that a deep model has considerably more linear regions that a shallow one. We consider this as a first step towards understanding the complexity of these models and specifically towards providing suitable mathematical tools for future analysis.},
archivePrefix = {arXiv},
arxivId = {1312.6098},
author = {Pascanu, Razvan and Montufar, Guido and Bengio, Yoshua},
eprint = {1312.6098},
journal = {Neural Information Processing Systems},
keywords = {artificial neural network,deep learning,hyperplane ar-,rangement,rectifier unit,representational power},
pages = {1--17},
title = {{On the number of response regions of deep feed forward networks with piece-wise linear activations}},
url = {https://arxiv.org/pdf/1312.6098.pdf http://arxiv.org/abs/1312.6098},
year = {2013}
}
@article{Mirsky2016,
author = {Mirsky, Steve},
doi = {10.1038/scientificamerican0616-78},
issn = {0036-8733},
journal = {Scientific American},
month = {may},
number = {6},
pages = {78--78},
title = {{Arms Race}},
url = {http://www.nature.com/doifinder/10.1038/scientificamerican0616-78},
volume = {314},
year = {2016}
}
@article{Palatucci2009,
abstract = {We consider the problem of zero-shot learning, where the goal is to$\backslash$nlearn a clas- sifier f : X �?Y that must predict novel values of$\backslash$nY that were omitted from the training set. To achieve this, we define$\backslash$nthe notion of a semantic output code classifier (SOC) which utilizes$\backslash$na knowledge base of semantic properties of Y to extrapolate to novel$\backslash$nclasses. We provide a formalism for this type of classifier and study$\backslash$nits theoretical properties in a PAC framework, showing conditions$\backslash$nun- der which the classifier can accurately predict novel classes.$\backslash$nAs a case study, we build a SOC classifier for a neural decoding$\backslash$ntask and show that it can often predict words that people are thinking$\backslash$nabout from functional magnetic resonance images (fMRI) of their neural$\backslash$nactivity, even without training examples for those words.},
author = {Palatucci, Mark and Hinton, Geoffrey E and Pomerleau, Dean and Mitchell, Tom M},
isbn = {9781615679119},
issn = {{\textless}null{\textgreater}},
journal = {Neural Information Processing Systems},
keywords = {Machine learning,zero-shot learning},
pages = {1--9},
title = {{Zero-Shot Learning with Semantic Output Codes}},
url = {http://papers.nips.cc/paper/3650-zero-shot-learning-with-semantic-output-codes.pdf},
year = {2009}
}
@article{VanderMaaten2008,
abstract = {We present a new technique called " t-SNE " that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {van der Maaten, L J P and Hinton, G E},
journal = {Journal of Machine Learning Research},
keywords = {da mds projection visualization},
pages = {2579--2605},
title = {{Visualizing High-Dimensional Data Using t-SNE}},
url = {https://lvdmaaten.github.io/publications/papers/JMLR{\_}2008.pdf},
volume = {9},
year = {2008}
}
@misc{Retro,
booktitle = {Retrosheet},
title = {{Retrosheet Event Files}},
url = {http://www.retrosheet.org/game.htm},
urldate = {2017-04-08}
}
@article{Socher2013,
abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of objects, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. Then, a separate recognition model can be employed for each type. We demonstrate two strategies, the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.},
author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew Y},
journal = {Neural Information Processing Systems},
pages = {935--943},
title = {{Zero-shot learning through cross-modal transfer}},
url = {http://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer.pdf},
year = {2013}
}
@article{Johnson2016,
abstract = {We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-the-art results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.},
archivePrefix = {arXiv},
arxivId = {1611.04558},
author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'{e}}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1611.04558},
journal = {arXiv},
month = {nov},
pages = {1--16},
title = {{Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation}},
url = {http://arxiv.org/abs/1611.04558},
year = {2016}
}
@misc{WAR,
booktitle = {FanGraphs},
title = {{What is WAR?}},
url = {http://www.fangraphs.com/library/misc/war/},
urldate = {2017-04-08}
}
@misc{Spector2016,
author = {Spector, Jesse},
booktitle = {Sporting News},
title = {{Mike Trout vs. Bryce Harper: The SN50 rivalry that defines this generation of stars}},
url = {http://www.sportingnews.com/mlb/news/sn50-2016-best-baseball-players-mike-trout-bryce-harper/mk3kmorbiyhr1f7onb7t5pehq},
urldate = {2017-04-19},
year = {2016}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
isbn = {2150-8097},
issn = {10495258},
journal = {Neural Information Processing Systems},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
year = {2013}
}
@misc{Kory2015,
author = {Kory, Matthew},
booktitle = {VICE Sports},
title = {{Paul Goldschmidt Might Really Be This Good}},
url = {https://sports.vice.com/en{\_}us/article/paul-goldschmidt-might-really-be-this-good},
urldate = {2017-04-19},
year = {2015}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
isbn = {0162-8828 VO - 35},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
month = {aug},
number = {8},
pages = {1798--1828},
pmid = {23459267},
title = {{Representation learning: A review and new perspectives}},
url = {http://ieeexplore.ieee.org/document/6472238/},
volume = {35},
year = {2013}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
pmid = {18244602},
title = {{A Neural Probabilistic Language Model}},
url = {http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf},
volume = {3},
year = {2003}
}
@misc{Alcorn2016,
author = {Alcorn, Michae A.},
title = {{Learning to Coach Football}},
url = {https://sites.google.com/view/michaelaalcorn/blog/learning-to-coach-football},
urldate = {2017-04-19},
year = {2016}
}
